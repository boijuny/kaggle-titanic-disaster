{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-04T21:06:11.603646Z","iopub.execute_input":"2024-07-04T21:06:11.604198Z","iopub.status.idle":"2024-07-04T21:06:11.615699Z","shell.execute_reply.started":"2024-07-04T21:06:11.604163Z","shell.execute_reply":"2024-07-04T21:06:11.614341Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom typing import List, Tuple\n\nclass DataLoader:\n    def __init__(self, train_path: str, test_path: str):\n        self.train_path = train_path\n        self.test_path = test_path\n    \n    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        print(\"Loading data...\", end=\" \")\n        train_data = pd.read_csv(self.train_path)\n        test_data = pd.read_csv(self.test_path)\n        print(\"Done.\")\n        return train_data, test_data\n\nclass FeatureEngineer:\n    @staticmethod\n    def extract_title(name: str) -> str:\n        return name.split(',')[1].split('.')[0].strip()\n\n    @staticmethod\n    def group_titles(title: str) -> str:\n        if title in ['Mr', 'Miss', 'Mrs', 'Master']:\n            return title\n        elif title in ['Dr', 'Rev', 'Col', 'Major', 'Mlle', 'Ms', 'Lady', 'Sir', 'Mme', 'Capt', 'Countess', 'Don', 'Jonkheer']:\n            return 'Rare'\n        else:\n            return 'Other'\n    \n    @staticmethod\n    def engineer_features(data: pd.DataFrame) -> pd.DataFrame:\n        print(\"Starting feature engineering...\", end=\" \")\n        data['Title'] = data['Name'].apply(FeatureEngineer.extract_title).apply(FeatureEngineer.group_titles)\n        data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n        data['IsAlone'] = (data['FamilySize'] == 1).astype(int)\n        data['TicketFrequency'] = data.groupby('Ticket')['Ticket'].transform('count')\n        data['FarePerPerson'] = data['Fare'] / data['FamilySize']\n        \n        # Age and Fare Binning\n        data['AgeBin'] = pd.cut(data['Age'], bins=[0, 12, 18, 50, 80], labels=['Child', 'Teen', 'Adult', 'Senior'])\n        data['FareBin'] = pd.qcut(data['Fare'], 4, labels=['Low', 'Medium', 'High', 'VeryHigh'])\n        \n        # Extract Deck from Cabin\n        data['Deck'] = data['Cabin'].str[0]\n        \n        # Extract Ticket Prefix\n        data['TicketPrefix'] = data['Ticket'].apply(lambda x: x.split()[0] if not x.split()[0].isdigit() else 'None')\n        \n        print(\"Done.\")\n        return data\n\nclass Preprocessor:\n    @staticmethod\n    def handle_missing_values(data: pd.DataFrame) -> pd.DataFrame:\n        print(\"Handling missing values...\", end=\" \")\n        for column in ['Age', 'Embarked', 'Fare', 'Deck']:\n            if column in data.columns:\n                if column == 'Age':\n                    data[column] = data[column].fillna(data[column].median())\n                elif column == 'Embarked':\n                    data[column] = data[column].fillna(data[column].mode()[0])\n                elif column == 'Fare':\n                    data[column] = data[column].fillna(data[column].median())\n                elif column == 'Deck':\n                    data[column] = data[column].fillna('Unknown')\n        data = data.drop(columns=['Cabin', 'Name', 'Ticket'], errors='ignore')\n        print(\"Done.\")\n        return data\n    \n    @staticmethod\n    def encode_categorical(data: pd.DataFrame) -> pd.DataFrame:\n        print(\"Encoding categorical variables...\", end=\" \")\n        data = pd.get_dummies(data, columns=['Sex', 'Embarked', 'Title', 'AgeBin', 'FareBin', 'Deck', 'TicketPrefix'], dummy_na=True)\n        print(\"Done.\")\n        return data\n\nclass ModelTrainer:\n    def __init__(self, model, param_grid: dict):\n        self.model = model\n        self.param_grid = param_grid\n        self.best_model = None\n    \n    def train(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n        print(\"Starting model training with GridSearchCV...\")\n        grid_search = GridSearchCV(self.model, self.param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n        grid_search.fit(X_train, y_train)\n        self.best_model = grid_search.best_estimator_\n        print(\"Model training completed.\")\n        print(f\"Best parameters: {grid_search.best_params_}\")\n        print(f\"Best cross-validation score: {grid_search.best_score_}\")\n    \n    def validate(self, X_val: np.ndarray, y_val: np.ndarray) -> float:\n        print(\"Validating model...\", end=\" \")\n        y_pred = self.best_model.predict(X_val)\n        accuracy = accuracy_score(y_val, y_pred)\n        print(f\"Done. Validation Accuracy: {accuracy}\")\n        return accuracy\n    \n    def get_mispredictions(self, X_val: np.ndarray, y_val: np.ndarray, feature_names: List[str]) -> pd.DataFrame:\n        print(\"Getting mispredictions...\")\n        y_pred = self.best_model.predict(X_val)\n        X_val_df = pd.DataFrame(X_val, columns=feature_names)\n        X_val_df['True'] = y_val\n        X_val_df['Pred'] = y_pred\n        mispredictions = X_val_df[X_val_df['True'] != X_val_df['Pred']]\n        print(\"Mispredictions obtained.\")\n        return mispredictions\n\nclass TitanicPipeline:\n    def __init__(self, train_path: str, test_path: str, output_path: str):\n        self.train_path = train_path\n        self.test_path = test_path\n        self.output_path = output_path\n        self.train_data = None\n        self.test_data = None\n        self.model_trainer = None\n        self.scaler = StandardScaler()\n    \n    def run(self):\n        print(\"Pipeline started...\")\n        # Load data\n        data_loader = DataLoader(self.train_path, self.test_path)\n        self.train_data, self.test_data = data_loader.load_data()\n        \n        # Feature engineering\n        self.train_data = FeatureEngineer.engineer_features(self.train_data)\n        self.test_data = FeatureEngineer.engineer_features(self.test_data)\n        \n        # Preprocess data\n        self.train_data = Preprocessor.handle_missing_values(self.train_data)\n        self.train_data = Preprocessor.encode_categorical(self.train_data)\n        self.test_data = Preprocessor.handle_missing_values(self.test_data)\n        self.test_data = Preprocessor.encode_categorical(self.test_data)\n        \n        # Align data\n        print(\"Aligning train and test data...\", end=\" \")\n        self.train_data, self.test_data = self.train_data.align(self.test_data, join='left', axis=1, fill_value=0)\n        print(\"Done.\")\n        \n        # Ensure no missing values after alignment\n        print(\"Checking for missing values after alignment...\", end=\" \")\n        if self.train_data.isnull().values.any():\n            print(\"Found missing values in training data. Handling missing values again...\", end=\" \")\n            self.train_data = self.train_data.fillna(0)\n            print(\"Done.\")\n        if self.test_data.isnull().values.any():\n            print(\"Found missing values in test data. Handling missing values again...\", end=\" \")\n            self.test_data = self.test_data.fillna(0)\n            print(\"Done.\")\n        print(\"No missing values found.\")\n        \n        # Select features\n        print(\"Selecting features...\", end=\" \")\n        features = [col for col in self.train_data.columns if col not in ['PassengerId', 'Survived']]\n        X = self.train_data[features]\n        y = self.train_data['Survived']\n        print(\"Done.\")\n        \n        # Split data\n        print(\"Splitting data into training and validation sets...\", end=\" \")\n        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n        print(\"Done.\")\n        \n        # Scale features\n        print(\"Scaling features...\", end=\" \")\n        X_train = self.scaler.fit_transform(X_train)\n        X_val = self.scaler.transform(X_val)\n        X_test = self.scaler.transform(self.test_data[features])\n        print(\"Done.\")\n        \n        # Train model with GridSearchCV\n        param_grid = {\n            'n_estimators': [200],\n            'max_depth': [20],\n            'min_samples_split': [2],\n            'min_samples_leaf': [2],\n            'max_features': ['log2'],\n            'bootstrap': [False],\n            'criterion': ['gini']\n        }\n        self.model_trainer = ModelTrainer(RandomForestClassifier(random_state=42), param_grid)\n        self.model_trainer.train(X_train, y_train)\n        self.model_trainer.validate(X_val, y_val)\n        \n        # Ensure no missing values in test data\n        if np.isnan(X_test).any():\n            print(\"Columns with NaN values in test data:\", self.test_data.columns[self.test_data.isna().any()].tolist())\n            raise ValueError(\"Test data contains NaN values after preprocessing.\")\n\n        # Predict on test data\n        print(\"Generating predictions on test data...\", end=\" \")\n        test_predictions = self.model_trainer.best_model.predict(X_test)\n        print(\"Done.\")\n        \n        # Save submission\n        self.save_submission(test_predictions)\n        print(\"Pipeline completed.\")\n        \n        # Analyze mispredictions\n        analysis_tool = AnalysisTool()\n        mispredictions = analysis_tool.get_mispredictions(X_val, y_val, self.model_trainer, features)\n        print(\"Mispredictions: \\n\", mispredictions)\n    \n    def save_submission(self, predictions: np.ndarray) -> None:\n        print(f\"Saving submission to {self.output_path}...\", end=\" \")\n        submission = pd.DataFrame({\n            'PassengerId': self.test_data['PassengerId'],\n            'Survived': predictions\n        })\n        submission.to_csv(self.output_path, index=False)\n        print(f\"Done. Submission file saved as {self.output_path}\")\n\nclass AnalysisTool:\n    @staticmethod\n    def get_mispredictions(X_val: np.ndarray, y_val: np.ndarray, model_trainer: ModelTrainer, feature_names: List[str]) -> pd.DataFrame:\n        print(\"Analyzing mispredictions...\")\n        mispredictions = model_trainer.get_mispredictions(X_val, y_val, feature_names)\n        return mispredictions\n\n# Usage\npipeline = TitanicPipeline('/kaggle/input/titanic/train.csv', '/kaggle/input/titanic/test.csv', '/kaggle/working/submission.csv')\npipeline.run()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T21:06:11.617789Z","iopub.execute_input":"2024-07-04T21:06:11.618134Z","iopub.status.idle":"2024-07-04T21:06:15.656473Z","shell.execute_reply.started":"2024-07-04T21:06:11.618104Z","shell.execute_reply":"2024-07-04T21:06:15.653955Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stdout","text":"Pipeline started...\nLoading data... Done.\nStarting feature engineering... Done.\nStarting feature engineering... Done.\nHandling missing values... Done.\nEncoding categorical variables... Done.\nHandling missing values... Done.\nEncoding categorical variables... Done.\nAligning train and test data... Done.\nChecking for missing values after alignment... Found missing values in test data. Handling missing values again... Done.\nNo missing values found.\nSelecting features... Done.\nSplitting data into training and validation sets... Done.\nScaling features... Done.\nStarting model training with GridSearchCV...\nModel training completed.\nBest parameters: {'bootstrap': False, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\nBest cross-validation score: 0.8356643356643356\nValidating model... Done. Validation Accuracy: 0.8268156424581006\nGenerating predictions on test data... Done.\nSaving submission to /kaggle/working/submission.csv... Done. Submission file saved as /kaggle/working/submission.csv\nPipeline completed.\nAnalyzing mispredictions...\nGetting mispredictions...\nMispredictions obtained.\nMispredictions: \n        Pclass       Age     SibSp     Parch      Fare  FamilySize   IsAlone  \\\n0    0.813034 -0.092634  0.379923  0.784700 -0.333901    0.634859 -1.231219   \n1   -0.400551  0.138156 -0.470722 -0.479342 -0.425284   -0.554666  0.812203   \n2    0.813034 -0.708074 -0.470722 -0.479342 -0.474867   -0.554666  0.812203   \n3   -0.400551 -1.785093 -0.470722  0.784700  0.007966    0.040096 -1.231219   \n4    0.813034 -1.169653  0.379923 -0.479342 -0.411002    0.040096 -1.231219   \n..        ...       ...       ...       ...       ...         ...       ...   \n173  0.813034 -1.554303  0.379923  0.784700 -0.333901    0.634859 -1.231219   \n175  0.813034 -0.092634 -0.470722 -0.479342 -0.488346   -0.554666  0.812203   \n176  0.813034  0.676666  0.379923  5.840867 -0.023083    3.013909 -1.231219   \n177 -0.400551 -0.938863 -0.470722 -0.479342 -0.425284   -0.554666  0.812203   \n178  0.813034 -1.938953  0.379923  0.784700 -0.305899    0.634859 -1.231219   \n\n     TicketFrequency  FarePerPerson  Sex_female  ...  TicketPrefix_STON/O  \\\n0           0.128479      -0.394501   -0.724310  ...            -0.119352   \n1          -0.591812      -0.252472   -0.724310  ...            -0.119352   \n2          -0.591812      -0.319973   -0.724310  ...            -0.119352   \n3           0.848770      -0.095187    1.380624  ...            -0.119352   \n4           0.128479      -0.380374    1.380624  ...            -0.119352   \n..               ...            ...         ...  ...                  ...   \n173         0.128479      -0.394501    1.380624  ...            -0.119352   \n175        -0.591812      -0.338323   -0.724310  ...            -0.119352   \n176         1.569061      -0.410177    1.380624  ...            -0.119352   \n177        -0.591812      -0.252472    1.380624  ...            -0.119352   \n178         0.128479      -0.381794    1.380624  ...            -0.119352   \n\n     TicketPrefix_STON/O2.  TicketPrefix_SW/PP  TicketPrefix_W./C.  \\\n0                -0.092188           -0.037503           -0.092188   \n1                -0.092188           -0.037503           -0.092188   \n2                -0.092188           -0.037503           -0.092188   \n3                -0.092188           -0.037503           -0.092188   \n4                -0.092188           -0.037503           -0.092188   \n..                     ...                 ...                 ...   \n173              -0.092188           -0.037503           -0.092188   \n175              -0.092188           -0.037503           -0.092188   \n176              -0.092188           -0.037503           -0.092188   \n177              -0.092188           -0.037503           -0.092188   \n178              -0.092188           -0.037503           -0.092188   \n\n     TicketPrefix_W.E.P.  TicketPrefix_W/C  TicketPrefix_WE/P  \\\n0              -0.037503         -0.037503          -0.053074   \n1              -0.037503         -0.037503          -0.053074   \n2              -0.037503         -0.037503          -0.053074   \n3              -0.037503         -0.037503          -0.053074   \n4              -0.037503         -0.037503          -0.053074   \n..                   ...               ...                ...   \n173            -0.037503         -0.037503          -0.053074   \n175            -0.037503         -0.037503          -0.053074   \n176            -0.037503         -0.037503          -0.053074   \n177            -0.037503         -0.037503          -0.053074   \n178            -0.037503         -0.037503          -0.053074   \n\n     TicketPrefix_nan  True  Pred  \n0                 0.0   NaN     0  \n1                 0.0   NaN     0  \n2                 0.0   NaN     0  \n3                 0.0   NaN     1  \n4                 0.0   NaN     0  \n..                ...   ...   ...  \n173               0.0   NaN     1  \n175               0.0   NaN     0  \n176               0.0   NaN     0  \n177               0.0   NaN     1  \n178               0.0   NaN     1  \n\n[161 rows x 90 columns]\n","output_type":"stream"}]}]}